#!/usr/bin/env python3
"""
HSC-JIT v3.7 - Scraper Validation & Refinement System
======================================================

This script validates and refines Roland and Boss scraping to ensure:
1. 100% data extraction coverage
2. Robust autonomous background execution
3. Proper frontend data handoff
4. Recovery from failures

Usage:
    python validate_and_refine_scrapers.py --validate        # Validate current data
    python validate_and_refine_scrapers.py --refine          # Refine scrapers
    python validate_and_refine_scrapers.py --scrape-boss     # Scrape Boss products
    python validate_and_refine_scrapers.py --full-pipeline   # Full validation + scrape
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
import logging
import argparse

# Setup path
sys.path.insert(0, str(Path(__file__).parent))
from core.config import settings
from services.roland_scraper import RolandScraper
from services.boss_scraper import BossScraper
from services.moog_scraper import MoogScraper
from services.nord_scraper import NordScraper
from core.validator import CatalogValidator

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ScraperValidator:
    """Validate scraper output and data quality"""
    
    def __init__(self):
        self.validator = CatalogValidator()
    
    def validate_catalog(self, catalog_path: Path) -> Dict[str, Any]:
        """Comprehensive validation of catalog file"""
        logger.info(f"üîç Validating {catalog_path.name}...")
        
        if not catalog_path.exists():
            return {
                "exists": False,
                "error": "File not found"
            }
        
        try:
            with open(catalog_path) as f:
                data = json.load(f)
        except Exception as e:
            return {
                "exists": True,
                "error": f"Parse error: {e}"
            }
        
        # Validate using existing validator
        report = self.validator.validate(data)
        
        products = data.get('products', [])
        brand_id = data.get('brand_identity', {}).get('id', 'unknown')
        
        return {
            "exists": True,
            "brand_id": brand_id,
            "product_count": len(products),
            "total_products_field": data.get('total_products'),
            "is_valid": report.is_valid,
            "error_count": report.error_count,
            "warning_count": report.warning_count,
            "last_updated": data.get('last_updated'),
            "coverage": self._analyze_coverage(products),
            "issues": [str(i) for i in report.issues[:5]]  # First 5 issues
        }
    
    def _analyze_coverage(self, products: List[Dict]) -> Dict[str, Any]:
        """Analyze data coverage across products"""
        if not products:
            return {
                "total_products": 0,
                "products_with_images": 0,
                "products_with_specs": 0,
                "products_with_features": 0,
                "products_with_manuals": 0,
                "products_with_videos": 0,
                "total_images": 0,
                "total_specs": 0,
                "total_features": 0,
                "avg_images_per_product": 0
            }
        
        total = len(products)
        with_images = sum(1 for p in products if p.get('images'))
        with_specs = sum(1 for p in products if p.get('specifications'))
        with_features = sum(1 for p in products if p.get('features'))
        with_manuals = sum(1 for p in products if p.get('manual_urls'))
        with_videos = sum(1 for p in products if p.get('video_urls'))
        
        total_images = sum(len(p.get('images', [])) for p in products)
        total_specs = sum(len(p.get('specifications', [])) for p in products)
        total_features = sum(len(p.get('features', [])) for p in products)
        
        return {
            "total_products": total,
            "products_with_images": with_images,
            "image_coverage_percent": round(100 * with_images / total) if total > 0 else 0,
            "products_with_specs": with_specs,
            "spec_coverage_percent": round(100 * with_specs / total) if total > 0 else 0,
            "products_with_features": with_features,
            "feature_coverage_percent": round(100 * with_features / total) if total > 0 else 0,
            "products_with_manuals": with_manuals,
            "manual_coverage_percent": round(100 * with_manuals / total) if total > 0 else 0,
            "products_with_videos": with_videos,
            "video_coverage_percent": round(100 * with_videos / total) if total > 0 else 0,
            "total_images": total_images,
            "total_specs": total_specs,
            "total_features": total_features,
            "avg_images_per_product": round(total_images / total) if total > 0 else 0
        }
    
    def validate_all(self) -> Dict[str, Dict[str, Any]]:
        """Validate all brand catalogs"""
        results = {}
        
        for catalog_file in settings.CATALOGS_DIR.glob("*_catalog.json"):
            brand = catalog_file.stem.replace("_catalog", "")
            results[brand] = self.validate_catalog(catalog_file)
        
        return results


class ScraperRefinement:
    """Refine scrapers for better coverage and reliability"""
    
    @staticmethod
    def print_validation_report(results: Dict[str, Dict[str, Any]]):
        """Pretty print validation results"""
        print("\n" + "="*80)
        print("üìä SCRAPER VALIDATION REPORT")
        print("="*80 + "\n")
        
        for brand, result in results.items():
            print(f"üè∑Ô∏è  {brand.upper()}")
            print("-" * 80)
            
            if not result.get('exists'):
                print(f"   ‚ùå {result.get('error')}\n")
                continue
            
            print(f"   Product Count: {result['product_count']} / {result['total_products_field']}")
            print(f"   Status: {'‚úÖ VALID' if result['is_valid'] else '‚ùå INVALID'}")
            print(f"   Errors: {result['error_count']} | Warnings: {result['warning_count']}")
            print(f"   Last Updated: {result['last_updated']}")
            
            cov = result.get('coverage', {})
            print(f"\n   üìà Coverage:")
            print(f"      Images:   {cov.get('image_coverage_percent', 0)}% "
                  f"({cov.get('total_images', 0)} total)")
            print(f"      Specs:    {cov.get('spec_coverage_percent', 0)}% "
                  f"({cov.get('total_specs', 0)} total)")
            print(f"      Features: {cov.get('feature_coverage_percent', 0)}% "
                  f"({cov.get('total_features', 0)} total)")
            print(f"      Manuals:  {cov.get('manual_coverage_percent', 0)}% "
                  f"({cov.get('products_with_manuals', 0)} products)")
            print(f"      Videos:   {cov.get('video_coverage_percent', 0)}% "
                  f"({cov.get('products_with_videos', 0)} products)")
            
            if result['error_count'] > 0:
                print(f"\n   ‚ö†Ô∏è  Sample Issues:")
                for issue in result['issues'][:3]:
                    print(f"      - {issue}")
            
            print()
    
    @staticmethod
    async def scrape_brand(brand: str, max_products: int = None) -> bool:
        """Scrape a brand with error handling"""
        print(f"\nüéØ Starting {brand.upper()} scrape...")
        
        try:
            if brand.lower() == "roland":
                scraper = RolandScraper()
            elif brand.lower() == "boss":
                scraper = BossScraper()
            elif brand.lower() == "moog":
                scraper = MoogScraper()
            elif brand.lower() == "nord":
                scraper = NordScraper()
            else:
                logger.error(f"Unknown brand: {brand}")
                return False
            
            # Run scraper
            catalog = await scraper.scrape_all_products(max_products=max_products)
            
            # Save catalog
            settings.CATALOGS_DIR.mkdir(parents=True, exist_ok=True)
            catalog_path = settings.CATALOGS_DIR / f"{brand.lower()}_catalog.json"
            
            with open(catalog_path, 'w') as f:
                f.write(catalog.model_dump_json(indent=2))
            
            logger.info(f"‚úÖ Saved {brand} catalog to {catalog_path}")
            logger.info(f"   Products: {len(catalog.products)}")
            logger.info(f"   Total Images: {sum(len(p.images) for p in catalog.products)}")
            
            return True
        
        except Exception as e:
            logger.error(f"‚ùå Failed to scrape {brand}: {e}", exc_info=True)
            return False


async def main():
    parser = argparse.ArgumentParser(
        description="HSC-JIT v3.7 Scraper Validation & Refinement"
    )
    parser.add_argument(
        "--validate", 
        action="store_true",
        help="Validate current catalog data"
    )
    parser.add_argument(
        "--refine",
        action="store_true",
        help="Show refinement recommendations"
    )
    parser.add_argument(
        "--scrape-roland",
        action="store_true",
        help="Re-scrape Roland products"
    )
    parser.add_argument(
        "--scrape-boss",
        action="store_true",
        help="Scrape Boss products"
    )
    parser.add_argument(
        "--scrape-moog",
        action="store_true",
        help="Scrape Moog products"
    )
    parser.add_argument(
        "--scrape-nord",
        action="store_true",
        help="Scrape Nord products"
    )
    parser.add_argument(
        "--max-products",
        type=int,
        default=None,
        help="Maximum products to scrape (default: all)"
    )
    parser.add_argument(
        "--full-pipeline",
        action="store_true",
        help="Full validation + scraping pipeline"
    )
    
    args = parser.parse_args()
    
    # Default to validate if no args
    if not any([args.validate, args.refine, args.scrape_roland, 
                args.scrape_boss, args.scrape_moog, args.scrape_nord, args.full_pipeline]):
        args.validate = True
    
    validator = ScraperValidator()
    
    # Step 1: Validate current catalogs
    if args.validate or args.full_pipeline:
        print("\nüîç VALIDATION PHASE")
        print("=" * 80)
        results = validator.validate_all()
        ScraperRefinement.print_validation_report(results)
    
    # Step 2: Scrape
    if args.scrape_roland or args.full_pipeline:
        success = await ScraperRefinement.scrape_brand(
            "roland", 
            max_products=args.max_products
        )
        if success:
            # Re-validate
            print("\nüîç RE-VALIDATING ROLAND...")
            result = validator.validate_catalog(
                settings.CATALOGS_DIR / "roland_catalog.json"
            )
            ScraperRefinement.print_validation_report({"roland": result})
    
    if args.scrape_boss or args.full_pipeline:
        success = await ScraperRefinement.scrape_brand(
            "boss",
            max_products=args.max_products
        )
        if success:
            # Re-validate
            print("\nüîç RE-VALIDATING BOSS...")
            result = validator.validate_catalog(
                settings.CATALOGS_DIR / "boss_catalog.json"
            )
            ScraperRefinement.print_validation_report({"boss": result})
    
    if args.scrape_moog:
        success = await ScraperRefinement.scrape_brand(
            "moog",
            max_products=args.max_products
        )
        if success:
            # Re-validate
            print("\nüîç RE-VALIDATING MOOG...")
            result = validator.validate_catalog(
                settings.CATALOGS_DIR / "moog_catalog.json"
            )
            ScraperRefinement.print_validation_report({"moog": result})
    
    if args.scrape_nord:
        success = await ScraperRefinement.scrape_brand(
            "nord",
            max_products=args.max_products
        )
        if success:
            # Re-validate
            print("\nüîç RE-VALIDATING NORD...")
            result = validator.validate_catalog(
                settings.CATALOGS_DIR / "nord_catalog.json"
            )
            ScraperRefinement.print_validation_report({"nord": result})
    
    # Step 3: Recommendations
    if args.refine:
        print("\nüí° REFINEMENT RECOMMENDATIONS")
        print("=" * 80)
        print("""
‚úÖ ROLAND:
   - Data completeness: EXCELLENT (28 products, comprehensive coverage)
   - Action: Monitor for new products monthly

‚ö†Ô∏è  BOSS:
   - Status: NOT YET SCRAPED
   - Action: Run scraper with --scrape-boss flag
   - Expected: 50-100+ products

üöÄ FOR AUTONOMOUS OPERATION:
   1. Create background scheduler (see daemon_scraper.py)
   2. Implement error recovery and retries
   3. Set up monitoring and alerting
   4. Configure data sync to frontend
   5. Add periodic validation checks
        """)


if __name__ == "__main__":
    asyncio.run(main())
