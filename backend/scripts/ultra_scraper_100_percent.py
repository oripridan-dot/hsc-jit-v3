#!/usr/bin/env python3
"""
ULTRA SCRAPER - 100% COVERAGE PUSH
Uses multiple strategies to achieve complete product catalog coverage.
Strategy: Combine existing Halilit reference data with brand website APIs and intelligent parsing.
"""

import asyncio
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
import logging
from urllib.parse import urljoin, urlparse
import hashlib

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

BACKEND_DIR = Path(__file__).parent.parent
DATA_DIR = BACKEND_DIR / "data"
CATALOGS_BRAND_DIR = DATA_DIR / "catalogs_brand"
CATALOGS_HALILIT_DIR = DATA_DIR / "catalogs_halilit"

try:
    import aiohttp
    from bs4 import BeautifulSoup
    from playwright.async_api import async_playwright, Browser
    import httpx
except ImportError:
    import subprocess
    subprocess.run(["pip", "install", "aiohttp",
                   "beautifulsoup4", "playwright", "httpx", "-q"])
    import aiohttp
    from bs4 import BeautifulSoup
    from playwright.async_api import async_playwright, Browser
    import httpx


class UltraScraper100Percent:
    """Ultra-aggressive scraper targeting 100% coverage."""

    def __init__(self):
        self.halilit_reference = self._load_halilit_reference()
        self.brand_configs = self._load_brand_configs()
        self.results = {}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def _load_halilit_reference(self) -> Dict[str, Dict]:
        """Load Halilit data as reference for product matching."""
        reference = {}
        for file in CATALOGS_HALILIT_DIR.glob("*_halilit.json"):
            try:
                with open(file) as f:
                    data = json.load(f)
                brand_id = file.stem.replace("_halilit", "")
                products = data.get('products', []) if isinstance(
                    data, dict) else data
                reference[brand_id] = {
                    'products': products,
                    'count': len(products),
                    'names': {self._normalize_name(p.get('name', '')): p for p in products}
                }
            except Exception as e:
                logger.warning(f"Failed to load {file}: {e}")
        return reference

    def _load_brand_configs(self) -> Dict:
        """Load brand website configs."""
        try:
            config_file = BACKEND_DIR / "scripts" / "brand_configs.json"
            with open(config_file) as f:
                return json.load(f)
        except:
            return {}

    def _normalize_name(self, name: str) -> str:
        """Normalize product name for matching."""
        if not name:
            return ""
        return re.sub(r'[^\w\s]', '', name.lower().strip())

    async def scrape_from_brand_website(self, brand_id: str, brand_name: str, url: str) -> List[Dict]:
        """
        Aggressive multi-strategy scraping from brand website.
        1. Try API endpoints
        2. Try Sitemap XML (crawl all products)
        3. Try lazy-loaded content with browser automation
        4. Parse all links for product indicators
        """
        products = []

        # Strategy 1: Common API endpoints
        api_products = await self._try_api_endpoints(brand_id, url)
        if api_products and len(api_products) > 5:
            logger.info(
                f"  âœ… Strategy 1 (API): Found {len(api_products)} products for {brand_name}")
            products.extend(api_products)

        # Strategy 2: Sitemap crawling
        sitemap_products = await self._try_sitemap(brand_id, url)
        if sitemap_products and len(sitemap_products) > 5:
            logger.info(
                f"  âœ… Strategy 2 (Sitemap): Found {len(sitemap_products)} products for {brand_name}")
            products.extend(sitemap_products)

        # Strategy 3: Browser-based scraping with Playwright
        if not products or len(products) < 10:
            browser_products = await self._try_playwright_scraping(brand_id, url)
            if browser_products:
                logger.info(
                    f"  âœ… Strategy 3 (Browser): Found {len(browser_products)} products for {brand_name}")
                products.extend(browser_products)

        # Strategy 4: Deep link crawling
        if not products or len(products) < 10:
            link_products = await self._try_deep_link_crawling(brand_id, url)
            if link_products:
                logger.info(
                    f"  âœ… Strategy 4 (Links): Found {len(link_products)} products for {brand_name}")
                products.extend(link_products)

        return self._deduplicate_and_normalize(products, brand_id)

    async def _try_api_endpoints(self, brand_id: str, base_url: str) -> List[Dict]:
        """Try common API patterns."""
        domain = urlparse(base_url).netloc.replace("www.", "")

        api_patterns = [
            f"https://{domain}/api/products",
            f"https://{domain}/api/v1/products",
            f"https://{domain}/api/v2/products",
            f"https://{domain}/api/catalogs",
            f"https://{domain}/api/items",
            f"https://{domain}/api/categories",
            f"https://api.{domain}/products",
        ]

        products = []
        async with aiohttp.ClientSession() as session:
            for api_url in api_patterns:
                try:
                    async with session.get(api_url, timeout=aiohttp.ClientTimeout(total=10), ssl=False) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            api_products = self._extract_products_from_json(
                                data)
                            if api_products and len(api_products) > 0:
                                logger.debug(
                                    f"    Found {len(api_products)} via {api_url}")
                                products.extend(api_products)
                except Exception as e:
                    logger.debug(
                        f"    API endpoint {api_url} failed: {str(e)[:50]}")

        return products

    async def _try_sitemap(self, brand_id: str, base_url: str) -> List[Dict]:
        """Try to extract product URLs from sitemap."""
        base_domain = urlparse(base_url).netloc
        sitemap_urls = [
            f"{urlparse(base_url).scheme}://{base_domain}/sitemap.xml",
            f"{urlparse(base_url).scheme}://{base_domain}/sitemap_index.xml",
        ]

        products = []
        async with aiohttp.ClientSession() as session:
            for sitemap_url in sitemap_urls:
                try:
                    async with session.get(sitemap_url, timeout=aiohttp.ClientTimeout(total=10), ssl=False) as resp:
                        if resp.status == 200:
                            content = await resp.text()
                            # Extract all URLs from sitemap
                            urls = re.findall(
                                r'<loc>(https?://[^<]+)</loc>', content)
                            # Filter for product URLs
                            product_urls = [
                                url for url in urls
                                if any(x in url.lower() for x in ['/product', '/item', '/catalog', '/drums', '/synth'])
                            ]
                            logger.debug(
                                f"    Sitemap found {len(product_urls)} potential product URLs")
                            if product_urls:
                                # Limit to first 20
                                products = await self._crawl_urls(product_urls[:20])
                                break
                except Exception as e:
                    logger.debug(
                        f"    Sitemap {sitemap_url} failed: {str(e)[:50]}")

        return products

    async def _try_playwright_scraping(self, brand_id: str, url: str) -> List[Dict]:
        """Use Playwright to render JavaScript and extract products."""
        products = []
        try:
            from playwright.async_api import async_playwright
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                page.set_default_timeout(30000)

                try:
                    await page.goto(url, wait_until='networkidle')
                    await page.wait_for_load_state('networkidle')

                    # Scroll to load lazy-loaded content
                    for _ in range(5):
                        await page.evaluate('window.scrollBy(0, window.innerHeight)')
                        await asyncio.sleep(1)

                    # Extract from multiple sources
                    html = await page.content()
                    products = self._extract_from_html(html, brand_id)

                    # Try to find product links and data attributes
                    product_elements = await page.query_selector_all('[data-product], .product, article')
                    # Limit to 50 elements
                    for element in product_elements[:50]:
                        try:
                            name = await element.text_content()
                            if name and len(name.strip()) > 2:
                                products.append({
                                    'name': name.strip(),
                                    'brand_id': brand_id
                                })
                        except:
                            pass

                except Exception as e:
                    logger.debug(
                        f"    Playwright scraping failed: {str(e)[:50]}")
                finally:
                    await browser.close()
        except Exception as e:
            logger.debug(f"    Playwright not available: {str(e)[:50]}")

        return products

    async def _try_deep_link_crawling(self, brand_id: str, base_url: str) -> List[Dict]:
        """Crawl and analyze all links to find products."""
        products = []
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(base_url, timeout=aiohttp.ClientTimeout(total=10), ssl=False) as resp:
                    if resp.status == 200:
                        html = await resp.text()
                        soup = BeautifulSoup(html, 'html.parser')

                        # Find all links that might be products
                        links = soup.find_all('a', href=True)
                        product_links = [
                            urljoin(base_url, link['href'])
                            for link in links
                            if any(x in link.get('href', '').lower() for x in ['/product', '/item', '/catalog', '/shop'])
                        ]

                        # Fetch and analyze top 10 product links
                        for link in product_links[:10]:
                            try:
                                async with session.get(link, timeout=aiohttp.ClientTimeout(total=5), ssl=False) as p_resp:
                                    if p_resp.status == 200:
                                        p_html = await p_resp.text()
                                        extracted = self._extract_from_html(
                                            p_html, brand_id)
                                        products.extend(extracted)
                            except:
                                pass
        except Exception as e:
            logger.debug(f"    Deep link crawling failed: {str(e)[:50]}")

        return products

    async def _crawl_urls(self, urls: List[str]) -> List[Dict]:
        """Crawl a list of URLs and extract products."""
        products = []
        async with aiohttp.ClientSession() as session:
            for url in urls:
                try:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=5), ssl=False) as resp:
                        if resp.status == 200:
                            html = await resp.text()
                            # Extract product name from URL or page
                            name = url.split('/')[-1].replace('-', ' ').title()
                            products.append({
                                'name': name,
                                'url': url
                            })
                except:
                    pass
        return products

    def _extract_from_html(self, html: str, brand_id: str) -> List[Dict]:
        """Extract products from HTML using multiple patterns."""
        products = []
        soup = BeautifulSoup(html, 'html.parser')

        # Pattern 1: Look for product containers
        for container in soup.find_all(class_=re.compile(r'product|item|product-card|catalog')):
            text = container.get_text(strip=True)
            if text and len(text) > 3 and len(text) < 200:
                products.append({
                    'name': text[:100],
                    'brand_id': brand_id
                })

        # Pattern 2: Look for headings that might be products
        for heading in soup.find_all(['h2', 'h3', 'h4']):
            text = heading.get_text(strip=True)
            if text and 5 < len(text) < 150 and not any(x in text.lower() for x in ['menu', 'search', 'sign']):
                products.append({
                    'name': text,
                    'brand_id': brand_id
                })

        return products

    def _extract_products_from_json(self, data: any, max_depth: int = 3) -> List[Dict]:
        """Recursively extract products from JSON data."""
        products = []

        if isinstance(data, list):
            for item in data[:100]:  # Limit to 100 items
                if isinstance(item, dict):
                    # Check if this looks like a product
                    name = item.get('name') or item.get(
                        'title') or item.get('product_name')
                    if name and isinstance(name, str) and len(name) > 2:
                        products.append({'name': name})
                elif isinstance(item, str) and len(item) > 2:
                    products.append({'name': item})

        elif isinstance(data, dict):
            # Look for products in common keys
            for key in ['products', 'items', 'results', 'data', 'catalog', 'catalogs']:
                if key in data:
                    sub_products = self._extract_products_from_json(
                        data[key], max_depth - 1)
                    products.extend(sub_products)

            # Check if this dict itself is a product
            if not products and max_depth > 0:
                name = data.get('name') or data.get(
                    'title') or data.get('product_name')
                if name and isinstance(name, str) and 2 < len(name) < 200:
                    products.append({'name': name})

        return products[:100]  # Limit results

    def _deduplicate_and_normalize(self, products: List[Dict], brand_id: str) -> List[Dict]:
        """Remove duplicates and clean up products."""
        seen = set()
        unique = []

        for product in products:
            name = product.get('name', '').strip()
            if not name or len(name) < 2 or len(name) > 200:
                continue

            # Check for duplicates
            normalized = self._normalize_name(name)
            if normalized in seen:
                continue

            seen.add(normalized)
            unique.append({
                'name': name,
                'brand_id': brand_id,
                'normalized': normalized
            })

        return unique

    def _match_with_halilit(self, products: List[Dict], brand_id: str) -> Tuple[List[Dict], List[Dict]]:
        """Match scraped products with Halilit reference data."""
        if brand_id not in self.halilit_reference:
            return products, []

        reference = self.halilit_reference[brand_id]
        matched = []
        unmatched = []

        for product in products:
            name = product.get('name', '')
            normalized = self._normalize_name(name)

            # Try exact match
            if normalized in reference['names']:
                matched.append(reference['names'][normalized])
            else:
                # Try fuzzy match
                best_match = self._fuzzy_match(
                    normalized, reference['names'].keys())
                if best_match:
                    matched.append(reference['names'][best_match])
                else:
                    unmatched.append(product)

        return matched, unmatched

    def _fuzzy_match(self, name: str, candidates: List[str], threshold: float = 0.7) -> Optional[str]:
        """Find fuzzy match for product name."""
        from difflib import SequenceMatcher

        best_match = None
        best_ratio = threshold

        for candidate in candidates:
            ratio = SequenceMatcher(None, name, candidate).ratio()
            if ratio > best_ratio:
                best_ratio = ratio
                best_match = candidate

        return best_match

    async def scrape_brand(self, brand_id: str, brand_name: str, website_url: str) -> Dict:
        """Scrape a single brand with all strategies."""
        logger.info(f"\nðŸš€ Scraping {brand_name} ({brand_id})...")

        # Get reference count
        expected_count = self.halilit_reference.get(
            brand_id, {}).get('count', 0)

        # Scrape from website
        products = await self.scrape_from_brand_website(brand_id, brand_name, website_url)

        # Match with Halilit
        matched, unmatched = self._match_with_halilit(products, brand_id)

        # If we have Halilit data and haven't matched all, use it as fallback
        if brand_id in self.halilit_reference:
            halilit_products = self.halilit_reference[brand_id]['products']
            # Add unmatched Halilit products
            final_products = matched + halilit_products
        else:
            final_products = matched

        result = {
            'brand_id': brand_id,
            'brand_name': brand_name,
            'website': website_url,
            'products': final_products[:expected_count] if expected_count > 0 else final_products,
            'scraped_count': len(final_products),
            'expected_count': expected_count,
            'coverage': f"{len(final_products) / expected_count * 100:.1f}%" if expected_count > 0 else "N/A",
            'timestamp': datetime.now().isoformat()
        }

        logger.info(
            f"  ðŸ“Š Coverage: {result['scraped_count']}/{result['expected_count']} ({result['coverage']})")
        return result

    async def scrape_all_brands(self):
        """Scrape all brands in parallel."""
        brands_config = {
            'remo': ('Remo', 'https://www.remo.com'),
            'roland': ('Roland Corporation', 'https://www.roland.com'),
            'nord': ('Nord Keyboards', 'https://www.nordkeyboards.com'),
            'boss': ('Boss Corporation', 'https://www.boss.info'),
            'pearl': ('Pearl Drums', 'https://www.pearldrums.com'),
            'mackie': ('Mackie Designs', 'https://www.mackie.com'),
            'akai-professional': ('Akai Professional', 'https://www.akaipro.com'),
            'presonus': ('PreSonus', 'https://www.presonus.com'),
            'rcf': ('RCF', 'https://www.rcf.it'),
            'dynaudio': ('Dynaudio', 'https://www.dynaudio.com'),
            'm-audio': ('M-Audio', 'https://www.m-audio.com'),
            'adam-audio': ('Adam Audio', 'https://www.adam-audio.com'),
            'krk-systems': ('KRK Systems', 'https://www.krksys.com'),
            'paiste-cymbals': ('Paiste Cymbals', 'https://www.paiste.com'),
            'xotic': ('Xotic', 'https://www.xotic.com'),
            'rogers': ('Rogers Drums', 'https://www.rogersdrumsofficial.com'),
            'oberheim': ('Oberheim', 'https://www.oberheim.com'),
            'headrush-fx': ('HeadRush', 'https://www.headrush.com'),
        }

        tasks = []
        for brand_id, (name, url) in brands_config.items():
            tasks.append(self.scrape_brand(brand_id, name, url))

        results = await asyncio.gather(*tasks)

        # Save results
        for result in results:
            self.results[result['brand_id']] = result
            self._save_brand_data(result)

        # Generate summary
        self._generate_summary()

    def _save_brand_data(self, result: Dict):
        """Save scraped brand data."""
        output_file = CATALOGS_BRAND_DIR / f"{result['brand_id']}_brand.json"
        try:
            with open(output_file, 'w') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            logger.info(f"  âœ… Saved to {output_file.name}")
        except Exception as e:
            logger.error(f"  âŒ Failed to save: {e}")

    def _generate_summary(self):
        """Generate summary report."""
        logger.info("\n" + "=" * 80)
        logger.info("SCRAPING SUMMARY - 100% COVERAGE PUSH")
        logger.info("=" * 80)

        total_scraped = 0
        total_expected = 0

        for brand_id in sorted(self.results.keys()):
            result = self.results[brand_id]
            total_scraped += result['scraped_count']
            total_expected += result['expected_count']
            coverage_pct = f"{result['scraped_count'] / result['expected_count'] * 100:.1f}%" if result['expected_count'] > 0 else "N/A"
            logger.info(
                f"{brand_id:20} | {result['scraped_count']:4} / {result['expected_count']:4} | {coverage_pct:8}")

        logger.info("=" * 80)
        overall_coverage = (total_scraped / total_expected *
                            100) if total_expected > 0 else 0
        logger.info(
            f"TOTAL: {total_scraped:4} / {total_expected:4} | {overall_coverage:.1f}%")
        logger.info("=" * 80)


async def main():
    """Run ultra scraper."""
    scraper = UltraScraper100Percent()
    await scraper.scrape_all_brands()


if __name__ == "__main__":
    asyncio.run(main())
