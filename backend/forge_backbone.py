"""
ÔøΩ HALILIT CATALOG SYSTEM
=========================

The "Halilit Catalog" architecture: Pre-calculate everything.
Scrapes ‚Üí Raw Data ‚Üí Refiner ‚Üí Golden Record (Static JSON) ‚Üí Frontend

This script runs offline to build verified, static catalog data that the frontend consumes instantly.
No runtime API calls. No database queries. Just pristine JSON.

Usage:
    python3 forge_backbone.py
    
Result:
    frontend/public/data/ is populated with:
    - index.json (The Spine - Master Catalog Index)
    - <brand>.json (Individual Brand Catalogs)
"""

import json
import os
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Any
import logging
import urllib.request
import urllib.error
from io import BytesIO
import base64
from services.visual_factory import VisualFactory
from models.taxonomy_registry import TaxonomyRegistry, get_registry

# --- SETUP LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

# --- CONFIGURATION ---
SOURCE_DIR = Path("../data/catalogs_brand")  # Where scraper outputs live
PUBLIC_DATA_PATH = Path("../frontend/public/data")  # The "Live" destination
LOGOS_DIR = PUBLIC_DATA_PATH / "logos"  # Logo destination
CATALOG_VERSION = "3.7.4"

# Brand color themes (WCAG AA compliant)
BRAND_THEMES = {
    "roland": {
        "primary": "#f89a1c",  # Roland Orange
        "secondary": "#18181b",
        "accent": "#ffffff",
        "background": "#18181b",
        "text": "#ffffff"
    },
    "boss": {
        "primary": "#0055a4",  # Boss Blue
        "secondary": "#0f172a",
        "accent": "#bfdbfe",
        "background": "#020617",
        "text": "#ffffff"
    },
    "nord": {
        "primary": "#e31e24",  # Nord Red
        "secondary": "#450a0a",
        "accent": "#fbbf24",
        "background": "#450a0a",
        "text": "#ffffff"
    },
    "moog": {
        "primary": "#000000",
        "secondary": "#5c4033", # Wood
        "accent": "#22c55e",
        "background": "#1c1917",
        "text": "#e5e7eb"
    },
    "adam-audio": {
        "primary": "#000000",
        "secondary": "#1c1917",
        "accent": "#fee2e2", # Pale Red
        "background": "#000000",
        "text": "#ffffff"
    },
    "teenage-engineering": {
        "primary": "#e5e5e5",
        "secondary": "#ff4d00", # TE Orange
        "accent": "#000000",
        "background": "#f0f0f0",
        "text": "#000000"
    },
    "universal-audio": {
        "primary": "#1f2937",
        "secondary": "#111827",
        "accent": "#3b82f6",
        "background": "#000000",
        "text": "#f3f4f6"
    },
    "akai-professional": {
        "primary": "#ff0000", # Akai Red
        "secondary": "#000000",
        "accent": "#ffffff",
        "background": "#1a1a1a",
        "text": "#ffffff"
    },
    "warm-audio": {
        "primary": "#ea580c", # Warm Orange
        "secondary": "#431407",
        "accent": "#fb923c",
        "background": "#2a150d",
        "text": "#ffedd5"
    },
    "mackie": {
        "primary": "#00a651", # Mackie Green
        "secondary": "#000000",
        "accent": "#86efac",
        "background": "#101010",
        "text": "#ffffff"
    }
}


class HalilitCatalog:
    """The Halilit Catalog System - Transforms raw data into production-ready static JSON catalogs."""
    
    def __init__(self):
        self.source_dir = SOURCE_DIR
        self.output_dir = PUBLIC_DATA_PATH
        # Initialize Visual Factory
        self.visual_factory = VisualFactory()
        # Initialize Taxonomy Registry for category validation
        self.taxonomy_registry = get_registry()
        
        # Flat structure matching Frontend Interface (MasterIndex)
        self.master_index = {
            "version": CATALOG_VERSION,
            "build_timestamp": datetime.now(timezone.utc).isoformat(),
            "environment": "static_production",
            "total_products": 0,
            "total_verified": 0,
            "brands": [],
            "search_graph": []
        }
        self.stats = {
            "brands_processed": 0,
            "products_total": 0,
            "images_verified": 0,
            "errors": []
        }
    
    def _download_logo(self, logo_url: str, brand_slug: str) -> str:
        """
        Manage brand logos. 
        PRIORITY 1: Use local override file in /assets/logos/ (Manual VIP treatment)
        PRIORITY 2: Download from URL (Fallback)
        """
        # PRIORITY 1: Check Manual Local Overrides
        # In Docker workspace, we map paths relative to frontend/public
        # Path relative to PUBLIC_DATA_PATH (which is frontend/public/data)
        
        # We look in frontend/public/assets/logos
        assets_logo_dir = self.output_dir.parent / "assets" / "logos"
        known_logo_files = [
            f"{brand_slug}_logo.svg",
            f"{brand_slug}.svg",
            f"{brand_slug}_logo.png",
            f"{brand_slug}_logo.jpg",
            f"{brand_slug}_logo.jpeg",
            f"{brand_slug}_logo.webp"
        ]
        
        for filename in known_logo_files:
            local_file = assets_logo_dir / filename
            if local_file.exists():
                logger.info(f"       ‚≠ê Using local VIP logo for {brand_slug}: {filename}")
                return f"/assets/logos/{filename}"
                
        # PRIORITY 2: Download (Legacy fallback - largely unused now)
        if not logo_url or not logo_url.startswith('http'):
            return logo_url
        
        try:
            # Create logos directory
            logos_dir = self.output_dir / "logos"
            logos_dir.mkdir(parents=True, exist_ok=True)
            
            # Determine file extension from URL
            ext = '.svg' if '.svg' in logo_url else '.png'
            local_path = logos_dir / f"{brand_slug}_logo{ext}"
            
            # Skip if already downloaded
            if local_path.exists():
                return f"/data/logos/{brand_slug}_logo{ext}"
            
            # Download with timeout
            req = urllib.request.Request(
                logo_url,
                headers={'User-Agent': 'Mozilla/5.0 (Halilit Catalog Builder)'}
            )
            with urllib.request.urlopen(req, timeout=5) as response:
                logo_data = response.read()
                
                # Save locally
                with open(local_path, 'wb') as f:
                    f.write(logo_data)
                
                logger.info(f"      ‚úì Downloaded logo: {brand_slug} ({len(logo_data)} bytes)")
                return f"/data/logos/{brand_slug}_logo{ext}"
        
        except Exception as e:
            logger.warning(f"      ‚ö†Ô∏è  Failed to download logo from {logo_url}: {e}")
            # Fallback: return original URL and let browser fetch it
            return logo_url
    
    def build(self):
        """Main Catalog Build Process"""
        logger.info(f"üìö [CATALOG] Building Halilit Catalog v{CATALOG_VERSION}...")
        logger.info(f"   Source: {self.source_dir.absolute()}")
        logger.info(f"   Output: {self.output_dir.absolute()}")
        
        try:
            # 1. Prepare Workspace
            self._prepare_workspace()
            
            # 2. Export Taxonomy Registry (before processing brands)
            self._export_taxonomy()
            
            # 3. Process Each Brand
            self._forge_brands()
            
            # 4. Finalize Catalog
            self._finalize_catalog()
            
            # 5. Report
            self._report()
            
            logger.info("‚úÖ [CATALOG] Complete. System ready at frontend/public/data/index.json")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [CATALOG] Critical failure: {e}")
            return False
    
    def _export_taxonomy(self):
        """Export taxonomy registry to frontend for category navigation."""
        logger.info("   [1.5/5] Exporting Taxonomy Registry...")
        taxonomy_path = self.output_dir / "taxonomy.json"
        self.taxonomy_registry.export_to_frontend(taxonomy_path)
        logger.info(f"      ‚úì Taxonomy exported: {len(self.taxonomy_registry.get_all_brands())} brands")
    
    def _prepare_workspace(self):
        """Ensure output directory exists and is clean."""
        logger.info("   [1/4] Preparing catalog workspace...")
        
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True)
            logger.info(f"      Created {self.output_dir}")
        
        logger.info(f"      ‚úì Catalog workspace ready")
    
    def _forge_brands(self):
        """Process each brand catalog and generate static files."""
        logger.info("   [2/4] Building brand catalogs...")
        
        if not self.source_dir.exists():
            logger.warning(f"      Source directory {self.source_dir} does not exist")
            logger.info("      Checking for catalogs_unified instead...")
            alt_source = Path("./data/catalogs_unified")
            if alt_source.exists():
                self.source_dir = alt_source
            else:
                logger.warning("      No source data found. Using empty catalog.")
                return
        
        catalog_files = list(self.source_dir.glob("*.json"))
        
        if not catalog_files:
            logger.warning(f"      No JSON files found in {self.source_dir}")
            return
        
        # Filter out "-brand.json" and "_brand.json" files to avoid duplicates (only process catalogs)
        catalog_files = [f for f in catalog_files if not ('_brand.json' in f.name or '-brand.json' in f.name)]
        
        for catalog_file in catalog_files:
            try:
                self._process_brand(catalog_file)
            except Exception as e:
                logger.error(f"      ‚ùå Failed: {catalog_file.name} - {e}")
                self.stats["errors"].append(str(e))
    
    def _process_brand(self, catalog_file: Path):
        """Process a single brand catalog."""
        
        try:
            with open(catalog_file, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            # Extract Brand Info
            brand_name = raw_data.get('brand_name') or raw_data.get('name') or catalog_file.stem.replace('_', ' ').title()
            # Remove "Catalog" or "Brand" suffix from brand name for slug
            brand_name_clean = brand_name.replace(' Catalog', '').replace(' Brand', '').strip()
            safe_slug = brand_name_clean.lower().replace(" ", "-").replace(".", "").replace("&", "and")
            products = raw_data.get('products', [])
            product_count = len(products)
            
            logger.info(f"      üî® {brand_name:20s} ({product_count:3d} products) ‚Üí {safe_slug}.json")
            
            # --- REFINEMENT LAYER ---
            # Ensure data quality: IDs, images, structure
            refined_data = self._refine_brand_data(raw_data, brand_name, safe_slug)
            
            # --- BUILD SEARCH GRAPH ---
            # Build search index for instant search
            self._index_for_search(refined_data, safe_slug)
            
            # --- WRITE BRAND FILE ---
            # This is lazy-loaded when user clicks the brand
            output_file = self.output_dir / f"{safe_slug}.json"
            with open(output_file, 'w', encoding='utf-8') as out:
                json.dump(refined_data, out, indent=2, ensure_ascii=False)
            
            # --- UPDATE MASTER INDEX ---
            brand_identity = refined_data.get('brand_identity', {})
            brand_colors = brand_identity.get('brand_colors', {})
            
            self.master_index["brands"].append({
                "id": safe_slug,
                "name": brand_name,
                "slug": safe_slug,
                "count": product_count,
                # Frontend expects:
                "brand_color": brand_colors.get('primary'),
                "logo_url": brand_identity.get('logo_url'),
                "file": f"{safe_slug}.json",
                "data_file": f"{safe_slug}.json",
                "product_count": product_count,
                "verified_count": product_count,
                "last_updated": datetime.now(timezone.utc).isoformat()
            })
            
            # --- UPDATE STATS ---
            self.stats["brands_processed"] += 1
            self.stats["products_total"] += product_count
            
        except json.JSONDecodeError as e:
            logger.error(f"      ‚ùå Invalid JSON in {catalog_file.name}: {e}")
            self.stats["errors"].append(f"JSON error in {catalog_file.name}")
        except Exception as e:
            logger.error(f"      ‚ùå Error processing {catalog_file.name}: {e}")
            self.stats["errors"].append(str(e))
    
    def _refine_brand_data(self, raw_data: Dict, brand_name: str, slug: str) -> Dict:
        """
        Refinement Layer: Ensure data quality AND build hierarchy
        - Ensure all products have IDs
        - Validate image structure
        - Add missing fields
        - Build hierarchical category structure (main ‚Üí sub ‚Üí products)
        - Enrich brand_identity with theme colors
        - Download brand logos locally
        """
        
        refined = raw_data.copy()
        refined['brand_name'] = brand_name
        refined['brand_slug'] = slug
        refined['refined_at'] = datetime.now(timezone.utc).isoformat()
        
        # Enrich brand_identity with theme colors and download logo
        if 'brand_identity' not in refined:
            refined['brand_identity'] = {}

        # Critical: Ensure ID and Name exist for Schema Validation
        if 'id' not in refined['brand_identity']:
            refined['brand_identity']['id'] = slug
        if 'name' not in refined['brand_identity']:
            refined['brand_identity']['name'] = brand_name
        
        # Always set brand_colors from BRAND_THEMES
        # Try exact match first, then base brand name (e.g., 'roland-comprehensive' -> 'roland')
        base_slug = slug.replace('-comprehensive', '').replace('-catalog', '')
        refined['brand_identity']['brand_colors'] = BRAND_THEMES.get(slug) or BRAND_THEMES.get(base_slug, {})
        
        # Inherit logo from base brand if missing
        if not refined['brand_identity'].get('logo_url'):
             # Standard fallback logos
             known_logos = {
                 'roland': 'https://static.roland.com/assets/images/logo_roland.svg',
                 'boss': 'https://www.boss.info/static/boss_logo.svg',
                 'nord': 'https://www.nordkeyboards.com/sites/default/files/files/nord-logo.svg',
                 'moog': 'https://www.moogmusic.com/sites/default/files/moog_logo.svg',
                 'yamaha': 'https://jp.yamaha.com/sp/common/images/yamaha_logo.png'
             }
             if base_slug in known_logos:
                 refined['brand_identity']['logo_url'] = known_logos[base_slug]

        # Download logo and update path
        # Always attempt resolution to catch local overrides (assets/logos/*.svg)
        current_logo_url = refined['brand_identity'].get('logo_url')
        resolved_logo = self._download_logo(current_logo_url, slug)
        if resolved_logo:
            refined['brand_identity']['logo_url'] = resolved_logo
        
        # First pass: Ensure product quality and TAXONOMY VALIDATION
        taxonomy_stats = {"validated": 0, "normalized": 0, "uncategorized": 0}
        
        if 'products' in refined:
            for idx, product in enumerate(refined['products']):
                # Ensure ID
                if not product.get('id'):
                    product['id'] = f"{slug}-product-{idx}"
                
                # --- TAXONOMY VALIDATION CHECKPOINT ---
                raw_category = product.get('main_category') or product.get('category')
                raw_subcategory = product.get('subcategory')
                
                # Normalize main category using taxonomy registry
                normalized_category = self.taxonomy_registry.normalize_category(slug, raw_category)
                
                if normalized_category:
                    product['main_category'] = normalized_category
                    product['category'] = normalized_category
                    taxonomy_stats["validated"] += 1
                elif raw_category:
                    # Category exists but not in taxonomy - keep but mark
                    product['main_category'] = raw_category
                    product['category'] = raw_category
                    product['_taxonomy_warning'] = f"Category '{raw_category}' not in official taxonomy"
                    taxonomy_stats["normalized"] += 1
                else:
                    product['main_category'] = 'Uncategorized'
                    product['category'] = 'Uncategorized'
                    taxonomy_stats["uncategorized"] += 1
                
                # Normalize subcategory if present
                if raw_subcategory:
                    normalized_sub = self.taxonomy_registry.normalize_category(slug, raw_subcategory)
                    if normalized_sub:
                        product['subcategory'] = normalized_sub

                # Ensure images are lists
                if 'images' in product and isinstance(product['images'], dict):
                    product['images'] = [product['images']]
                elif 'images' not in product:
                    product['images'] = []
                
                # Ensure category_hierarchy
                if 'category_hierarchy' not in product:
                    product['category_hierarchy'] = [product.get('category', 'Uncategorized')]
                    if product.get('subcategory'):
                        product['category_hierarchy'].append(product['subcategory'])
                
                # --- NEW: VISUAL FACTORY INTEGRATION ---
                # Process Main Image into Thumbnail + Inspection Asset
                main_img_url = product.get('image_url') or product.get('image')
                if not main_img_url and product.get('images') and len(product['images']) > 0:
                    first_img = product['images'][0]
                    main_img_url = first_img.get('url') if isinstance(first_img, dict) else first_img

                # Handle pre-seeded local paths (Mock Data)
                if main_img_url and main_img_url.startswith('/data/'):
                     product['images'] = {
                        "main": main_img_url,
                        "thumbnail": main_img_url,
                        "high_res": main_img_url.replace('_thumb', '_main'),
                        "original": main_img_url
                    }
                     logger.info(f"      ‚è© Skipping visuals for local seed path: {main_img_url}")

                elif main_img_url and not main_img_url.startswith('http://localhost'): # Skip if already local (unlikely in forge)
                    # Prepare Output Path
                    # frontend/public/data/product_images/<brand>/<product_id>
                    img_output_dir = self.output_dir / "product_images" / slug
                    img_output_dir.mkdir(parents=True, exist_ok=True)
                    
                    img_base_path = str(img_output_dir / f"{product['id']}")
                    
                    # Run Visual Factory (This is heavy, maybe we cache check?)
                    # For now, we run it to ensure "Visual Intelligence" is active
                    logger.info(f"      üé® Processing visuals for {product.get('name')}...")
                    # BYPASS VISUAL FACTORY - RAW HARVEST MODE
                    # try:
                    #     visual_assets = self.visual_factory.process_product_asset(main_img_url, img_base_path)
                    
                    #     if visual_assets:
                    #         # Update product with new optimized local assets
                    #         # Convert absolute path to relative URL for frontend
                    #         # frontend/public/data/... -> /data/...
                    #         thumb_rel = f"/data/product_images/{slug}/{product['id']}_thumb.webp"
                    #         inspect_rel = f"/data/product_images/{slug}/{product['id']}_inspect.webp"
                            
                    #         product['images'] = {
                    #             "main": thumb_rel,          # Used by TierBar and default view
                    #             "thumbnail": thumb_rel,     # Explicit thumbnail
                    #             "high_res": inspect_rel,    # Used by InspectionLens through 'main' or separate field
                    #             "original": main_img_url    # Keep reference
                    #         }
                            
                    #         # Set primary image for legacy compatibility
                    #         product['image'] = thumb_rel
                    #         product['image_url'] = thumb_rel
                            
                    #         self.stats['images_verified'] += 1
                    # except Exception as e:
                    #      logger.warning(f"      ‚ö†Ô∏è Visual Factory failed for {product.get('name')}: {e}")
                    
                    # Fallback to remote URL if no local processing
                    if main_img_url:
                        product['image'] = main_img_url
                        product['image_url'] = main_img_url

                # --- NEW: DOWNLOAD INNER LOGOS (series_logo) ---
                if product.get('series_logo'):
                    # Create a unique name: roland-fantom-06-series.png
                    logo_name = f"{slug}-{product.get('id', idx)}-series"
                    local_path = self._download_logo(product['series_logo'], logo_name)
                    product['series_logo'] = local_path
                    logger.info(f"      ‚¨áÔ∏è  Downloaded inner logo for {product.get('name')}")
                
                # Data quality ensured - no unused AI layers
        
        # Log taxonomy validation stats
        logger.info(f"      üìä Taxonomy: {taxonomy_stats['validated']} validated, {taxonomy_stats['normalized']} normalized, {taxonomy_stats['uncategorized']} uncategorized")
        
        # Add taxonomy stats to brand_identity for frontend reference
        refined['brand_identity']['taxonomy_stats'] = taxonomy_stats
        
        # Add available categories from taxonomy registry for this brand
        brand_taxonomy = self.taxonomy_registry.get_brand(slug)
        if brand_taxonomy:
            refined['brand_identity']['categories'] = [
                {
                    "id": cat.id,
                    "label": cat.label,
                    "icon": cat.icon,
                    "description": cat.description,
                    "children": cat.children,
                }
                for cat in brand_taxonomy.get_root_categories()
            ]
        
        # Second pass: Build hierarchical category tree
        refined['hierarchy'] = self._build_category_hierarchy(refined.get('products', []))
        
        return refined
    
    def _build_category_hierarchy(self, products: List[Dict]) -> Dict:
        """
        Build a tree structure: Category ‚Üí Subcategory ‚Üí Products
        
        Structure:
        {
          "Electronics": {
            "Keyboards": [product1, product2],
            "Drums": [product3]
          },
          "Accessories": {
            "Cables": [product4]
          }
        }
        """
        hierarchy = {}
        
        for product in products:
            main_cat = product.get('main_category', 'Uncategorized')
            sub_cat = product.get('subcategory', 'General')
            
            # Ensure category exists
            if main_cat not in hierarchy:
                hierarchy[main_cat] = {}
            
            # Ensure subcategory exists
            if sub_cat not in hierarchy[main_cat]:
                hierarchy[main_cat][sub_cat] = []
            
            # Add product to subcategory (include full product data)
            hierarchy[main_cat][sub_cat].append({
                "id": product.get('id'),
                "name": product.get('name'),
                "description": product.get('short_description', product.get('description', '')),
                "images": product.get('images', []),
                "model_number": product.get('model_number'),
                "sku": product.get('sku')
            })
        
        return hierarchy
    
    def _index_for_search(self, brand_data: Dict, slug: str):
        """
        Build lightweight search graph entry for Halilit Catalog.
        This is what the navigator uses for instant suggestions.
        """
        
        brand_name = brand_data.get('brand_name', slug)
        
        for product in brand_data.get('products', []):
            entry = {
                "id": product.get('id'),
                "label": product.get('name', ''),
                "brand": slug,
                "brand_name": brand_name,
                "category": product.get('main_category', 'Uncategorized'),
                "subcategory": product.get('subcategory', 'General'),
                "keywords": product.get('features', [])[:5] if product.get('features') else [],
                "description": product.get('description', '')[:100] if product.get('description') else ''
            }
            self.master_index["search_graph"].append(entry)
    
    def _finalize_catalog(self):
        """Write the master index (The Spine of the Halilit Catalog)."""
        logger.info("   [3/4] Finalizing catalog structure...")
        
        # Update metadata
        self.master_index["total_brands"] = len(self.master_index["brands"])
        self.master_index["total_products"] = self.stats["products_total"]
        self.master_index["total_verified"] = self.stats["products_total"] # Assuming all generated items are verified
        
        # Write index.json (The Master Catalog File)
        index_file = self.output_dir / "index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(self.master_index, f, indent=2, ensure_ascii=False)
        
        logger.info(f"      ‚úì Master Catalog Index: {index_file.name}")
        logger.info(f"      ‚úì {len(self.master_index['brands'])} brands")
        logger.info(f"      ‚úì {self.master_index['total_products']} products")
        logger.info(f"      ‚úì {len(self.master_index['search_graph'])} search entries")
    
    def _report(self):
        """Print final catalog build report."""
        logger.info("   [4/4] Catalog Build Report")
        logger.info(f"      üìä Brands Processed:   {self.stats['brands_processed']}")
        logger.info(f"      üìä Total Products:     {self.stats['products_total']}")
        logger.info(f"      üìä Search Entries:     {len(self.master_index['search_graph'])}")
        
        if self.stats['errors']:
            logger.warning(f"      ‚ö†Ô∏è  Errors Encountered: {len(self.stats['errors'])}")
            for error in self.stats['errors'][:3]:
                logger.warning(f"         - {error}")
        else:
            logger.info(f"      ‚úÖ Zero Errors")
        
        logger.info("")
        logger.info("üéØ HALILIT CATALOG IS READY")
        logger.info(f"   Frontend can now fetch /data/index.json")
        logger.info(f"   Each brand lazy-loads from /data/<slug>.json")


if __name__ == "__main__":
    catalog = HalilitCatalog()
    success = catalog.build()
    exit(0 if success else 1)
