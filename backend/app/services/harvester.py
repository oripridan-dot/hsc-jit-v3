"""
THE HARVESTER: Universal Brand Catalog Builder

Purpose: Use configs generated by the Diplomat to scrape and populate catalogs.
This is the "Fuel Pump" that fills backend/data/catalogs/*.json with product data.

Architecture:
- Reads scrape_config.json from backend/data/brands/{brand}/
- Crawls pages using those rules
- Outputs to backend/data/catalogs/{brand}_catalog.json (format that CatalogService expects)

Usage (as script):
    python scripts/harvester.py --brand roland

Usage (as service):
    from app.services.harvester import HarvesterService
    harvester = HarvesterService()
    await harvester.harvest_brand("roland")
"""

import asyncio
import json
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin, urlparse
import httpx
from bs4 import BeautifulSoup
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HarvesterService:
    """Universal scraper that uses Diplomat-generated configs."""

    def __init__(self, data_dir: Optional[Path] = None):
        if data_dir is None:
            # Default: backend/data/ (two levels up from this file)
            backend_dir = Path(__file__).resolve().parents[2]
            data_dir = backend_dir / "data"

        self.data_dir = Path(data_dir)
        self.brands_dir = self.data_dir / "brands"
        self.catalogs_dir = self.data_dir / "catalogs"

        # Ensure directories exist
        self.brands_dir.mkdir(parents=True, exist_ok=True)
        self.catalogs_dir.mkdir(parents=True, exist_ok=True)

        self.headers = {
            "User-Agent": "Mozilla/5.0 (compatible; HSC-JIT-Harvester/3.4)"
        }

        # Brand-specific category definitions (from brand websites, not Halilit)
        self.brand_categories = self._load_brand_categories()

    def _load_brand_categories(self) -> Dict[str, List[str]]:
        """
        Load brand-specific category definitions.
        These represent the brand's own product categories, not Halilit's.
        Maps brand_id -> list of categories from brand website
        """
        return {
            "roland": ["keyboards", "synthesizers", "drums", "guitars", "audio-interfaces", "effects", "stage-pianos"],
            "nord": ["keyboards", "synthesizers", "expansions", "drum-machines"],
            "moog": ["synthesizers", "keyboards", "effects", "controllers"],
            "korg": ["synthesizers", "keyboards", "production", "drums", "effects", "controllers"],
            "yamaha": ["keyboards", "synthesizers", "audio-interfaces", "monitors", "drums"],
            "mxl": ["microphones", "stands", "cables", "accessories"],
            "shure": ["microphones", "wireless", "headphones", "accessories"],
            "boss": ["effects", "multi-effects", "drums", "loopers", "metronomes"],
            "behringer": ["audio-interfaces", "mixers", "power-amplifiers", "effects", "keyboards"],
            "akai-professional": ["midi-controllers", "production-equipment", "sampling"],
            "m-audio": ["audio-interfaces", "midi-controllers", "keyboards", "monitors"],
            "presonus": ["audio-interfaces", "mixers", "monitors", "software"],
            "adam-audio": ["studio-monitors", "headphones"],
            "krk-systems": ["studio-monitors", "headphones"],
            "dynaudio": ["studio-monitors", "headphones"],
            "oberheim": ["synthesizers", "keyboards"],
            "headrush-fx": ["guitar-effects", "multi-effects"],
            "native-instruments": ["production-software", "keyboards", "controllers"],
            "electro-harmonix": ["vacuum-tubes", "pedals"],
        }

    async def harvest_brand(self, brand_id: str, max_pages: int = 5) -> Dict[str, Any]:
        """
        Main entry point: Harvest products for a brand using its config.

        Returns:
            {
                "success": bool,
                "brand_id": str,
                "products_found": int,
                "catalog_path": str,
                "error": str (if failed)
            }
        """
        logger.info(f"üåæ [HARVESTER] Starting harvest for: {brand_id}")

        # Load config
        config_path = self.brands_dir / brand_id / "scrape_config.json"
        if not config_path.exists():
            error_msg = f"Config not found: {config_path}. Run diplomat.py first."
            logger.error(error_msg)
            return {"success": False, "brand_id": brand_id, "error": error_msg}

        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        # Extract products
        products = await self._scrape_products(config, max_pages)

        if not products:
            logger.warning(f"‚ö†Ô∏è No products found for {brand_id}")
            return {
                "success": False,
                "brand_id": brand_id,
                "products_found": 0,
                "error": "No products extracted"
            }

        # Build catalog in CatalogService format
        catalog = self._build_catalog(brand_id, products, config)

        # Save to catalogs directory
        catalog_path = self.catalogs_dir / f"{brand_id}_catalog.json"
        with open(catalog_path, "w", encoding="utf-8") as f:
            json.dump(catalog, f, indent=2, ensure_ascii=False)

        logger.info(
            f"‚úÖ [HARVESTER] Saved {len(products)} products to {catalog_path}")

        return {
            "success": True,
            "brand_id": brand_id,
            "products_found": len(products),
            "catalog_path": str(catalog_path)
        }

    async def _scrape_products(self, config: Dict[str, Any], max_pages: int) -> List[Dict[str, Any]]:
        """
        Crawl pages and extract products using config rules.
        """
        base_url = config.get("base_url", "")
        product_list_selector = config.get("product_list_selector")
        product_item_selector = config.get("product_item_selector")
        fields_config = config.get("fields", {})
        pagination_config = config.get("pagination", {})

        all_products = []
        current_url = base_url
        pages_scraped = 0

        async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:
            while current_url and pages_scraped < max_pages:
                logger.info(
                    f"üìÑ [HARVESTER] Scraping page {pages_scraped + 1}: {current_url}")

                try:
                    response = await client.get(current_url, headers=self.headers)
                    response.raise_for_status()
                    html = response.text
                except Exception as e:
                    logger.error(f"Failed to fetch {current_url}: {e}")
                    break

                soup = BeautifulSoup(html, 'html.parser')

                # Extract products from this page
                products = self._extract_products_from_page(
                    soup,
                    product_list_selector,
                    product_item_selector,
                    fields_config,
                    base_url
                )

                all_products.extend(products)
                logger.info(
                    f"  ‚úì Extracted {len(products)} products (total: {len(all_products)})")

                pages_scraped += 1

                # Check for next page
                next_url = self._get_next_page_url(
                    soup, pagination_config, current_url)
                if not next_url or next_url == current_url:
                    break

                current_url = next_url
                await asyncio.sleep(1)  # Be polite

        return all_products

    def _extract_products_from_page(
        self,
        soup: BeautifulSoup,
        list_selector: Optional[str],
        item_selector: str,
        fields_config: Dict[str, Any],
        base_url: str
    ) -> List[Dict[str, Any]]:
        """
        Extract products from a single page using CSS selectors.
        """
        products = []

        # Guard against None/empty selectors - try universal fallbacks
        if not item_selector:
            logger.warning(
                f"Product item selector is None or empty - trying universal patterns")
            item_selector = self._find_product_selector(soup)
            if not item_selector:
                logger.warning(
                    "Could not find products with universal patterns")
                return []

        # Find product items
        if list_selector:
            container = soup.select_one(list_selector)
            if not container:
                logger.warning(
                    f"Product list container not found: {list_selector}")
                # Try without container
                items = soup.select(item_selector)
            else:
                items = container.select(item_selector)
        else:
            items = soup.select(item_selector)

        if not items:
            logger.warning(
                f"No product items found with selector: {item_selector}")
            return []

        # Extract fields from each item
        for item in items:
            product = {}

            for field_name, field_config in fields_config.items():
                value = self._extract_field_value(
                    item, field_name, field_config, base_url)
                product[field_name] = value

            # Only add if we got at least a name
            if product.get("name"):
                products.append(product)

        return products

    def _find_product_selector(self, soup: BeautifulSoup) -> Optional[str]:
        """
        Try to auto-detect product items using common patterns.
        Returns the best selector found, or None.
        """
        # Common product selectors in order of specificity
        patterns = [
            '.product-item',
            '.product-card',
            '.product',
            '[data-product-id]',
            '[data-product]',
            'article.product',
            '.item-card',
            '.product-list-item',
            'li[class*="product"]',
            'div[class*="product"]',
        ]

        for pattern in patterns:
            items = soup.select(pattern)
            # Need at least 2 items to be confident it's a product list
            if len(items) >= 2:
                logger.info(
                    f"Auto-detected product selector: {pattern} ({len(items)} items)")
                return pattern

        return None

    def _extract_field_value(self, item: BeautifulSoup, field_name: str, field_config: Dict[str, Any], base_url: str) -> Optional[str]:
        """
        Extract a single field value from a product item.
        Handles special cases like name extraction with fallbacks.
        """
        selector = field_config.get("selector", "")
        attribute = field_config.get("attribute", "text")

        if not selector:
            return None

        element = item.select_one(selector)
        if not element:
            # Auto-detect for name field
            if field_name == "name" and not element:
                # Try common name selectors
                for sel in ['h2', 'h3', 'h4', '.name', '.title', '.product-name']:
                    element = item.select_one(sel)
                    if element and element.get_text(strip=True):
                        break

            if not element:
                return None

        # Extract value based on attribute type
        if attribute == "text":
            value = element.get_text(strip=True)
        elif attribute == "src" or attribute == "href":
            value = element.get(attribute, "")
            # Handle data-src for lazy-loaded images
            if attribute == "src" and not value:
                value = element.get(
                    "data-src", "") or element.get("data-original", "")
            # Make relative URLs absolute
            if value and not value.startswith("http"):
                value = urljoin(base_url, value)
        else:
            value = element.get(attribute, "")

        return value

    def _get_next_page_url(
        self,
        soup: BeautifulSoup,
        pagination_config: Dict[str, Any],
        current_url: str
    ) -> Optional[str]:
        """
        Find the next page URL based on pagination config.
        """
        pagination_type = pagination_config.get("type", "none")

        if pagination_type == "none":
            return None

        if pagination_type == "button":
            selector = pagination_config.get("next_button_selector")
            if selector:
                next_button = soup.select_one(selector)
                if next_button:
                    next_url = next_button.get("href")
                    if next_url and not next_url.startswith("http"):
                        parsed = urlparse(current_url)
                        base_url = f"{parsed.scheme}://{parsed.netloc}"
                        next_url = urljoin(base_url, next_url)
                    return next_url

        # Add more pagination types as needed (numbered, infinite scroll, etc.)

        return None

    def _build_catalog(self, brand_id: str, products: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Convert raw scraped products into CatalogService format.

        NOTE: Uses brand-defined categories from self.brand_categories, NOT Halilit's.
        This ensures each brand's own product categorization is respected.
        """
        # Build brand identity
        brand_name = brand_id.replace("-", " ").title()

        # Get brand's category definitions (from brand website, not Halilit)
        brand_category_list = self.brand_categories.get(brand_id, [])

        brand_identity = {
            "id": brand_id,
            "name": brand_name,
            "website": config.get("base_url", ""),
            "logo_url": None,
            "hq": None,
            "categories": brand_category_list,  # Brand website categories
            "category_mapping": self._build_category_mapping(brand_category_list)
        }

        # Transform products
        # IMPORTANT: Override category with brand-appropriate category
        catalog_products = []
        for idx, raw_product in enumerate(products):
            # Generate product ID
            product_name = raw_product.get("name") or f"product-{idx}"
            product_id = f"{brand_id}-{self._slugify(product_name)}"

            # Normalize category to brand category
            # If product has no category or we have brand categories, use brand category
            normalized_category = self._normalize_to_brand_category(
                raw_product.get("category"),
                brand_id
            )

            # Build product entry - USE NORMALIZED BRAND CATEGORY
            product = {
                "id": product_id,
                "brand_id": brand_id,
                "brand": brand_id,  # For CatalogService filtering
                "name": raw_product.get("name", "Unknown Product"),
                "images": {
                    "main": raw_product.get("image_url"),
                    "thumbnail": raw_product.get("image_url")
                },
                "category": normalized_category,  # Normalized to brand category
                "price": raw_product.get("price"),
            }

            # Add documentation if detail URL exists
            if raw_product.get("detail_url"):
                product["documentation"] = {
                    "url": raw_product["detail_url"],
                    "type": "html"
                }

            catalog_products.append(product)

        return {
            "brand_identity": brand_identity,
            "products": catalog_products
        }

    def _normalize_to_brand_category(self, halilit_category: Optional[str], brand_id: str) -> str:
        """
        Convert Halilit's Hebrew category to the brand's English category.
        Falls back to a generic category if no mapping exists.
        """
        # If no category provided, use default
        if not halilit_category:
            return "Products"

        # Get brand categories
        brand_categories = self.brand_categories.get(brand_id, [])
        if not brand_categories:
            return "Products"

        # For now, just use the first brand category as default
        # In a real system, you'd have a more sophisticated category mapping
        return brand_categories[0] if brand_categories else "Products"

    def _build_category_mapping(self, categories: List[str]) -> Dict[str, str]:
        """
        Build a mapping of category display names to slugs.
        Useful for frontend normalization.
        """
        mapping = {}
        for cat in categories:
            if cat:
                # Keep original display name, create slug version
                slug = self._slugify(cat)
                mapping[cat] = slug
        return mapping

    def _slugify(self, text: str) -> str:
        """Convert text to URL-safe slug."""
        import re
        if not text:
            return "unknown"
        text = text.lower()
        text = re.sub(r'[^\w\s-]', '', text)
        text = re.sub(r'[-\s]+', '-', text)
        return text[:50]  # Limit length

    def get_harvest_status(self, brand_id: str) -> Dict[str, Any]:
        """
        Check if a brand has been harvested and get stats.
        """
        catalog_path = self.catalogs_dir / f"{brand_id}_catalog.json"
        config_path = self.brands_dir / brand_id / "scrape_config.json"

        return {
            "brand_id": brand_id,
            "has_config": config_path.exists(),
            "has_catalog": catalog_path.exists(),
            "config_path": str(config_path) if config_path.exists() else None,
            "catalog_path": str(catalog_path) if catalog_path.exists() else None,
            "product_count": self._get_product_count(catalog_path) if catalog_path.exists() else 0
        }

    def _get_product_count(self, catalog_path: Path) -> int:
        """Get product count from a catalog file."""
        try:
            with open(catalog_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict) and "products" in data:
                    return len(data["products"])
                elif isinstance(data, list):
                    return len(data)
        except Exception:
            pass
        return 0


# Script entry point
async def main():
    import argparse
    parser = argparse.ArgumentParser(
        description="Harvest products for a brand")
    parser.add_argument("--brand", required=True,
                        help="Brand identifier (e.g., roland)")
    parser.add_argument("--max-pages", type=int, default=5,
                        help="Maximum pages to scrape")
    args = parser.parse_args()

    harvester = HarvesterService()
    result = await harvester.harvest_brand(args.brand, max_pages=args.max_pages)

    if result["success"]:
        print(
            f"\n‚úÖ Successfully harvested {result['products_found']} products")
        print(f"üìÅ Catalog saved to: {result['catalog_path']}")
    else:
        print(f"\n‚ùå Harvest failed: {result.get('error')}")
        exit(1)


if __name__ == "__main__":
    asyncio.run(main())
