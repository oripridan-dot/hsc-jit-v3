"""
THE HARVESTER: Universal Brand Catalog Builder

Purpose: Use configs generated by the Diplomat to scrape and populate catalogs.
This is the "Fuel Pump" that fills backend/data/catalogs/*.json with product data.

Architecture:
- Reads scrape_config.json from backend/data/brands/{brand}/
- Crawls pages using those rules
- Outputs to backend/data/catalogs/{brand}_catalog.json (format that CatalogService expects)

Usage (as script):
    python scripts/harvester.py --brand roland

Usage (as service):
    from app.services.harvester import HarvesterService
    harvester = HarvesterService()
    await harvester.harvest_brand("roland")
"""

import asyncio
import json
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin, urlparse
import httpx
from bs4 import BeautifulSoup
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HarvesterService:
    """Universal scraper that uses Diplomat-generated configs."""

    def __init__(self, data_dir: Optional[Path] = None):
        if data_dir is None:
            # Default: backend/data/ (two levels up from this file)
            backend_dir = Path(__file__).resolve().parents[2]
            data_dir = backend_dir / "data"

        self.data_dir = Path(data_dir)
        self.brands_dir = self.data_dir / "brands"
        self.catalogs_dir = self.data_dir / "catalogs"

        # Ensure directories exist
        self.brands_dir.mkdir(parents=True, exist_ok=True)
        self.catalogs_dir.mkdir(parents=True, exist_ok=True)

        self.headers = {
            "User-Agent": "Mozilla/5.0 (compatible; HSC-JIT-Harvester/3.4)"
        }

    async def harvest_brand(self, brand_id: str, max_pages: int = 5) -> Dict[str, Any]:
        """
        Main entry point: Harvest products for a brand using its config.

        Returns:
            {
                "success": bool,
                "brand_id": str,
                "products_found": int,
                "catalog_path": str,
                "error": str (if failed)
            }
        """
        logger.info(f"üåæ [HARVESTER] Starting harvest for: {brand_id}")

        # Load config
        config_path = self.brands_dir / brand_id / "scrape_config.json"
        if not config_path.exists():
            error_msg = f"Config not found: {config_path}. Run diplomat.py first."
            logger.error(error_msg)
            return {"success": False, "brand_id": brand_id, "error": error_msg}

        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        # Extract products
        products = await self._scrape_products(config, max_pages)

        if not products:
            logger.warning(f"‚ö†Ô∏è No products found for {brand_id}")
            return {
                "success": False,
                "brand_id": brand_id,
                "products_found": 0,
                "error": "No products extracted"
            }

        # Build catalog in CatalogService format
        catalog = self._build_catalog(brand_id, products, config)

        # Save to catalogs directory
        catalog_path = self.catalogs_dir / f"{brand_id}_catalog.json"
        with open(catalog_path, "w", encoding="utf-8") as f:
            json.dump(catalog, f, indent=2, ensure_ascii=False)

        logger.info(
            f"‚úÖ [HARVESTER] Saved {len(products)} products to {catalog_path}")

        return {
            "success": True,
            "brand_id": brand_id,
            "products_found": len(products),
            "catalog_path": str(catalog_path)
        }

    async def _scrape_products(self, config: Dict[str, Any], max_pages: int) -> List[Dict[str, Any]]:
        """
        Crawl pages and extract products using config rules.
        """
        base_url = config.get("base_url", "")
        product_list_selector = config.get("product_list_selector")
        product_item_selector = config.get("product_item_selector")
        fields_config = config.get("fields", {})
        pagination_config = config.get("pagination", {})

        all_products = []
        current_url = base_url
        pages_scraped = 0

        async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:
            while current_url and pages_scraped < max_pages:
                logger.info(
                    f"üìÑ [HARVESTER] Scraping page {pages_scraped + 1}: {current_url}")

                try:
                    response = await client.get(current_url, headers=self.headers)
                    response.raise_for_status()
                    html = response.text
                except Exception as e:
                    logger.error(f"Failed to fetch {current_url}: {e}")
                    break

                soup = BeautifulSoup(html, 'html.parser')

                # Extract products from this page
                products = self._extract_products_from_page(
                    soup,
                    product_list_selector,
                    product_item_selector,
                    fields_config,
                    base_url
                )

                all_products.extend(products)
                logger.info(
                    f"  ‚úì Extracted {len(products)} products (total: {len(all_products)})")

                pages_scraped += 1

                # Check for next page
                next_url = self._get_next_page_url(
                    soup, pagination_config, current_url)
                if not next_url or next_url == current_url:
                    break

                current_url = next_url
                await asyncio.sleep(1)  # Be polite

        return all_products

    def _extract_products_from_page(
        self,
        soup: BeautifulSoup,
        list_selector: Optional[str],
        item_selector: str,
        fields_config: Dict[str, Any],
        base_url: str
    ) -> List[Dict[str, Any]]:
        """
        Extract products from a single page using CSS selectors.
        """
        products = []

        # Find product items
        if list_selector:
            container = soup.select_one(list_selector)
            if not container:
                logger.warning(
                    f"Product list container not found: {list_selector}")
                return []
            items = container.select(item_selector)
        else:
            items = soup.select(item_selector)

        if not items:
            logger.warning(
                f"No product items found with selector: {item_selector}")
            return []

        # Extract fields from each item
        for item in items:
            product = {}

            for field_name, field_config in fields_config.items():
                selector = field_config.get("selector", "")
                attribute = field_config.get("attribute", "text")

                if not selector:
                    continue

                element = item.select_one(selector)
                if not element:
                    product[field_name] = None
                    continue

                # Extract value based on attribute type
                if attribute == "text":
                    value = element.get_text(strip=True)
                elif attribute == "src" or attribute == "href":
                    value = element.get(attribute, "")
                    # Handle data-src for lazy-loaded images
                    if attribute == "src" and not value:
                        value = element.get("data-src", "")
                    # Make relative URLs absolute
                    if value and not value.startswith("http"):
                        value = urljoin(base_url, value)
                else:
                    value = element.get(attribute, "")

                product[field_name] = value

            # Only add if we got at least a name
            if product.get("name"):
                products.append(product)

        return products

    def _get_next_page_url(
        self,
        soup: BeautifulSoup,
        pagination_config: Dict[str, Any],
        current_url: str
    ) -> Optional[str]:
        """
        Find the next page URL based on pagination config.
        """
        pagination_type = pagination_config.get("type", "none")

        if pagination_type == "none":
            return None

        if pagination_type == "button":
            selector = pagination_config.get("next_button_selector")
            if selector:
                next_button = soup.select_one(selector)
                if next_button:
                    next_url = next_button.get("href")
                    if next_url and not next_url.startswith("http"):
                        parsed = urlparse(current_url)
                        base_url = f"{parsed.scheme}://{parsed.netloc}"
                        next_url = urljoin(base_url, next_url)
                    return next_url

        # Add more pagination types as needed (numbered, infinite scroll, etc.)

        return None

    def _build_catalog(self, brand_id: str, products: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Convert raw scraped products into CatalogService format.

        Expected format:
        {
            "brand_identity": {
                "brand_id": "roland",
                "brand_name": "Roland",
                "website": "https://www.roland.com",
                ...
            },
            "products": [
                {
                    "id": "roland-juno-x",
                    "brand_id": "roland",
                    "name": "JUNO-X",
                    "image_url": "...",
                    "documentation": {"url": "...", "type": "pdf"},
                    ...
                }
            ]
        }
        """
        # Build brand identity
        brand_name = brand_id.replace("-", " ").title()
        brand_identity = {
            "id": brand_id,  # CatalogService looks for "id" not "brand_id"
            "name": brand_name,
            "website": config.get("base_url", ""),
            "logo_url": None,  # Could be enhanced later
            "hq": None,
        }

        # Transform products
        catalog_products = []
        for idx, raw_product in enumerate(products):
            # Generate product ID
            product_name = raw_product.get("name", f"product-{idx}")
            product_id = f"{brand_id}-{self._slugify(product_name)}"

            # Build product entry
            product = {
                "id": product_id,
                "brand_id": brand_id,
                "brand": brand_id,  # For CatalogService filtering
                "name": raw_product.get("name", "Unknown Product"),
                "images": {
                    "main": raw_product.get("image_url"),
                    "thumbnail": raw_product.get("image_url")
                },
                "category": raw_product.get("category"),
                "price": raw_product.get("price"),
            }

            # Add documentation if detail URL exists
            if raw_product.get("detail_url"):
                product["documentation"] = {
                    "url": raw_product["detail_url"],
                    "type": "html"  # Will be fetched by ContentFetcher
                }

            catalog_products.append(product)

        return {
            "brand_identity": brand_identity,
            "products": catalog_products
        }

    def _slugify(self, text: str) -> str:
        """Convert text to URL-safe slug."""
        import re
        text = text.lower()
        text = re.sub(r'[^\w\s-]', '', text)
        text = re.sub(r'[-\s]+', '-', text)
        return text[:50]  # Limit length

    def get_harvest_status(self, brand_id: str) -> Dict[str, Any]:
        """
        Check if a brand has been harvested and get stats.
        """
        catalog_path = self.catalogs_dir / f"{brand_id}_catalog.json"
        config_path = self.brands_dir / brand_id / "scrape_config.json"

        return {
            "brand_id": brand_id,
            "has_config": config_path.exists(),
            "has_catalog": catalog_path.exists(),
            "config_path": str(config_path) if config_path.exists() else None,
            "catalog_path": str(catalog_path) if catalog_path.exists() else None,
            "product_count": self._get_product_count(catalog_path) if catalog_path.exists() else 0
        }

    def _get_product_count(self, catalog_path: Path) -> int:
        """Get product count from a catalog file."""
        try:
            with open(catalog_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict) and "products" in data:
                    return len(data["products"])
                elif isinstance(data, list):
                    return len(data)
        except Exception:
            pass
        return 0


# Script entry point
async def main():
    import argparse
    parser = argparse.ArgumentParser(
        description="Harvest products for a brand")
    parser.add_argument("--brand", required=True,
                        help="Brand identifier (e.g., roland)")
    parser.add_argument("--max-pages", type=int, default=5,
                        help="Maximum pages to scrape")
    args = parser.parse_args()

    harvester = HarvesterService()
    result = await harvester.harvest_brand(args.brand, max_pages=args.max_pages)

    if result["success"]:
        print(
            f"\n‚úÖ Successfully harvested {result['products_found']} products")
        print(f"üìÅ Catalog saved to: {result['catalog_path']}")
    else:
        print(f"\n‚ùå Harvest failed: {result.get('error')}")
        exit(1)


if __name__ == "__main__":
    asyncio.run(main())
