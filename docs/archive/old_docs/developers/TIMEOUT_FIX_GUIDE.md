# Timeout Fix Implementation Summary

## üéØ Problem

The Roland scraper was getting stuck indefinitely during:

- Category discovery loops
- Product URL collection
- Individual product page scraping
- Image/video/spec extraction

## ‚úÖ Solution

Added comprehensive `asyncio.wait_for()` timeouts at every async operation.

## üìç Key Timeout Locations

### 1. Navigation (Global)

```python
async def _navigate(self, page: Page, url: str, timeout: int = None):
    await asyncio.wait_for(
        page.goto(url, wait_until='domcontentloaded', timeout=timeout),
        timeout=20
    )
```

### 2. Category Discovery

```python
await asyncio.wait_for(
    self._navigate(page, cat_url),
    timeout=15
)

links = await asyncio.wait_for(
    page.locator('a[href*="/categories/"]').all(),
    timeout=10
)
```

### 3. Product URL Collection

```python
await asyncio.wait_for(
    self._navigate(page, self.products_url),
    timeout=20
)

main_page_links = await asyncio.wait_for(
    page.locator('a[href*="/products/"]').all(),
    timeout=10
)
```

### 4. Individual Product Scraping

```python
# Wrapper for entire product
product = await asyncio.wait_for(
    self._scrape_product_page(page, url),
    timeout=45  # 45 seconds per product max
)

# Navigation
await asyncio.wait_for(
    self._navigate(page, url),
    timeout=20
)

# Name extraction
if await asyncio.wait_for(page.locator('h1').count(), timeout=5) > 0:
    name = await asyncio.wait_for(
        page.locator('h1').first.inner_text(),
        timeout=5
    )
```

### 5. Element Extraction

```python
# Description
elements = await asyncio.wait_for(
    page.locator(selector).all(),
    timeout=5
)
for elem in elements:
    text = await asyncio.wait_for(elem.inner_text(), timeout=2)

# Images
img_elements = await asyncio.wait_for(
    page.locator(selector).all(),
    timeout=5
)
src = await asyncio.wait_for(img_elem.get_attribute('src'), timeout=2)
alt = await asyncio.wait_for(img_elem.get_attribute('alt'), timeout=2)

# Videos
elements = await asyncio.wait_for(
    page.locator(selector).all(),
    timeout=5
)
video_url = await asyncio.wait_for(
    elem.get_attribute('src'),
    timeout=2
)

# Specifications
tables = await asyncio.wait_for(
    page.locator('table').all(),
    timeout=5
)
```

### 6. Accessories (Complex)

```python
async def extract_accessories():
    response = await page.goto(accessories_url, timeout=5000)

    accessory_links = await asyncio.wait_for(
        page.locator('a[href*="/products/"]').all(),
        timeout=5
    )

    href = await asyncio.wait_for(link.get_attribute('href'), timeout=2)
    acc_name = await asyncio.wait_for(link.inner_text(), timeout=2)

# Wrap entire function
await asyncio.wait_for(extract_accessories(), timeout=20)
```

## ‚öôÔ∏è Configuration Changes

### Before

```python
SCRAPER_TIMEOUT: int = 30000  # 30 seconds
SCRAPER_RETRIES: int = 3
SCRAPER_RETRY_DELAY: int = 2
```

### After

```python
SCRAPER_TIMEOUT: int = 15000  # 15 seconds (reduced)
SCRAPER_RETRIES: int = 2       # reduced for faster failure
SCRAPER_RETRY_DELAY: int = 1   # reduced delay
```

## üö¶ Timeout Hierarchy

```
Global Pipeline (unlimited, but controlled by sub-timeouts)
‚îú‚îÄ Product Discovery (~60s total)
‚îÇ  ‚îú‚îÄ Category Discovery (15s per category)
‚îÇ  ‚îî‚îÄ URL Collection (10s per page)
‚îÇ
‚îî‚îÄ Product Scraping (45s per product)
   ‚îú‚îÄ Navigation (20s)
   ‚îú‚îÄ Name Extraction (5s)
   ‚îú‚îÄ Description (5s per selector, 2s per element)
   ‚îú‚îÄ Images (5s per selector, 2s per element)
   ‚îú‚îÄ Videos (5s per selector, 2s per element)
   ‚îú‚îÄ Specifications (5s)
   ‚îî‚îÄ Accessories (20s total)
```

## üîç Exception Handling Pattern

```python
try:
    result = await asyncio.wait_for(
        async_operation(),
        timeout=N
    )
except asyncio.TimeoutError:
    logger.warning(f"Timeout after {N}s, continuing...")
    # Graceful fallback or continue
except Exception as e:
    logger.error(f"Error: {e}")
    # Handle other errors
```

## ‚úÖ Testing Commands

```bash
# Quick test (3 products)
cd backend && python3 test_roland_scraper.py

# Development (10-15 products)
cd backend && python3 orchestrate_brand.py --brand roland --clean --max-products 15

# Production (full catalog)
cd backend && python3 orchestrate_brand.py --brand roland --clean
```

## üìä Expected Results

| Scenario    | Products | Time      | Success Rate |
| ----------- | -------- | --------- | ------------ |
| Test Suite  | 3        | ~85s      | 100%         |
| Development | 15       | ~2-3 min  | 90-100%      |
| Production  | 500+     | 60-90 min | 90-95%       |

## üéØ Key Takeaways

1. **Every async operation needs a timeout**
2. **Nest timeouts for complex operations**
3. **Use shorter base timeouts** (15s vs 30s)
4. **Graceful fallback on timeout** (don't crash)
5. **Log timeouts as warnings** (expected behavior)
6. **Use `domcontentloaded`** instead of `networkidle`

## üîí Guaranteed Behaviors

- ‚úÖ No operation can hang indefinitely
- ‚úÖ Browser always closes (even on error)
- ‚úÖ One failure doesn't stop pipeline
- ‚úÖ Clear timeout messages in logs
- ‚úÖ Predictable max runtime per product
